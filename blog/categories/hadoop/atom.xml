<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | 秦风汉雨]]></title>
  <link href="http://youli9056.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://youli9056.github.io/"/>
  <updated>2014-11-17T11:06:29+08:00</updated>
  <id>http://youli9056.github.io/</id>
  <author>
    <name><![CDATA[You Li]]></name>
    <email><![CDATA[youli9056@126.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce中的负载均衡问题二  详细分析及现有方案  Analyzing Load Balancing on Hadoop]]></title>
    <link href="http://youli9056.github.io/blog/analyzing-load-balancing-on-hadoop/"/>
    <updated>2014-11-12T16:18:21+08:00</updated>
    <id>http://youli9056.github.io/blog/analyzing-load-balancing-on-hadoop</id>
    <content type="html"><![CDATA[<p>在<a href="/blog/load-balancing-on-hadoop-mapreduce/">上一篇文章</a>中，简要介绍了<a href="/blog/load-balancing-on-hadoop-mapreduce/">负载均衡问题</a>，及其在Hadoop MapReduce平台中的表现。本文将进行详细分析，并介绍一些解决方案。</p>

<h2>一、倾斜类型</h2>

<ol>
<li><p>数据倾斜</p>

<p> 输入数据分布不均，有些节点处理的数据远大于其他节点。可能会出现下面的情况，有个Join操作（reduce端join）输出数据量的大小</p></li>
</ol>


<p><img src="/images/analyzingloadbalancing/reducesidejoin.png" alt="reduce side join output" /></p>

<ol>
<li><p>计算倾斜</p>

<p> 运算的复杂度与输入的数据量关系不大，而取决于输入数据的特性。例如，PageRank这样的算法，一个节点的计算复杂度是于这个节点的出度有关，即使分配到一个计算节点上的数据相等，它们的实际运算时间由于度数的差别，还是会出现类似下图这样的情况。</p></li>
</ol>


<p><img src="/images/analyzingloadbalancing/computeunbalance.png" alt="compute unbalance" /></p>

<h2>二、解决方案</h2>

<p>两种倾斜方式中数据倾斜看起来会比较容易理解些，其实也是较易解决的。而计算倾斜发生得也很频繁，且情况更严重，解决起来也比较麻烦。解决方法整体上有两种分类：静态负载均衡，动态负载均衡。</p>

<h3>2.1 静态负载均衡</h3>

<p>严格意义上讲，静态负载均衡大多是进行算法优化，改变用户程序，使得整个作业的执行时间尽可能降低。这样的负载均衡一般都是需要对于当前的作业任务、输入数据特征和各个机器节点资源有先验知识。作业开始运行前，哪块数据在哪里进行计算都是可预测的，或者说是确定的，不管运行多少次都是这样一个状态。Hadoop提供的Partition接口就可以看做是一种静态负载均衡。下图展示了一个静态负载均衡的效果。</p>

<p><img src="/images/analyzingloadbalancing/staticloadbalance.png" alt="static load banlance" /></p>

<p>静态均衡很难用来解决计算倾斜的情形。大量的输入数据，想对每个分块的数据做计算量的估计是一个十分繁复且巨大的工作，而且没有通用性。</p>

<h3>2.2 动态负载均衡</h3>

<p>Hadoop平台中以Slot作为计算资源的分配单位，map任务和reduce任务都是运行在Slot上。Slot可以理解为单位计算资源，与cpu对应。由于存在数据倾斜或者计算倾斜每个任务的运行时间不同，可能会出现某个Slot已经完成任务空闲了，其他的Slot上还有大量的任务没有完成。显而易见，如果此时能够把其他Slot的任务放到这个Slot上来运行会大大提高系统的资源使用率，同时能够提升任务的处理效率及系统的吞吐量。相比于静态负载均衡，动态的均衡有多个优点。它不需要改变用户自己的应用代码，也不要针对不同的输入数据做算法定制优化，系统能够自动地完成均衡工作。</p>

<p>动态均衡还有个优点，它可以用来解决计算倾斜的问题。动态均衡关注的是Slot的空闲与否，可以在运行时根据当前运行状况做出负载分配决策。</p>

<p>相对而言动态均衡比静态均衡更难实现些，为了实现任务间的数据迁移工作需要增加任务间的数据传输模块及其他辅助功能。</p>

<h3>2.3 关键技术</h3>

<p>负载均衡过程中涉及到三个基本问题：Which，Where，How。</p>

<p>Which&ndash; 目前系统中哪个节点的任务被当做Straggler，谁的任务重，需要被迁移。</p>

<p>Where&ndash; 系统中哪个节点任务轻，可以多承担些工作，将重的任务迁移到这里。</p>

<p>How&ndash; Straggler节点的任务如何迁移到空闲节点中去，而且保证系统的一致性、正确性。</p>

<p><strong>静态</strong></p>

<p>静态Which：对输入数据进行采样，分析作业，估算每个部分的代价（归约到时间运行值）超出均值的那些块为Straggler。</p>

<p>静态Where：估值低于均值的块为要迁移任务到的空闲点。</p>

<p>静态How：类似背包问题，但分配目标是各个背包的估计代价方差最小。</p>

<p><strong>动态</strong></p>

<p>动态Which：目前系统中出现空Slot时，正在运行的task都可以作为Straggler。从其中挑选一个作为Straggler，如何挑选也是通过代价估计。</p>

<p>动态Where：即当前空闲的Slot，将任务迁移到这个Slot。</p>

<p>动态How：
    A. 直接法：选择当前系统运行中task剩余时间最长的为Straggler，考虑迁移时间，使Straggler和空闲节点处理迁移任务的时间一致。
    B. 迭代法：考虑系统I/O问题，选择当前在同一个机器上的剩余时间最长的task为straggler，切分一半计算量到空闲节点；本地不够分再向其他机器请求切分。</p>

<p><img src="/images/analyzingloadbalancing/dynamicdemo.png" alt="dynamic demo" /></p>

<p>大概就像这样。</p>

<h2>三、代价估计模型</h2>

<p>上面提到的两种方案都需要有个代价估计，一半都是以计算时间作为标量。我们一般使用代价估计模型将不同的应用及对应的数据来将具体的作业情况做代价估计。</p>

<h3>3.1 Map task估计</h3>

<h3>3.2 Reduce task估计</h3>

<h3>3.3 具体设计</h3>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop MapReduce中的负载均衡问题 Load Balancing on Hadoop MapReduce]]></title>
    <link href="http://youli9056.github.io/blog/load-balancing-on-hadoop-mapreduce/"/>
    <updated>2014-11-08T11:34:58+08:00</updated>
    <id>http://youli9056.github.io/blog/load-balancing-on-hadoop-mapreduce</id>
    <content type="html"><![CDATA[<p>做了一段时间的Hadoop中的负载均衡问题，此文谨当做对那段时间的总结。</p>

<h2>一、负载均衡 &ndash; 一个广泛而普遍存在的问题</h2>

<p>负载均衡问题是一个广泛而普遍存在的问题。在所有的分布式系统中几乎都会提及到“长尾问题(Long Tail Problem)”，其实也就是大家常说的“短板理论”，系统的整体表现取决于表现最差的一部分。常见的分布式系统如分布式缓存，分布式存储，分布式计算，分布式数据库等等，都存在这个问题。分布式缓存中可能会遇到短时间内集中访问同一个缓存的情况；分布式存储可能单机磁盘使用过度；分布式计算可能会有单点的计算负担过重；分布式数据库可能会有单机访问量过大。如此总总，只要是分布的，想完全端平一碗水几乎是不大可能的。</p>

<p>在此，总结我对负载均衡的定义：在多点协作的系统中由于不合理的任务分配导致某个或者少量的某些节点处理负担过重，最终拖延整个系统对外的响应效率。</p>

<p>负载均衡的解决思路主要有：</p>

<pre><code>1. 被动解决，发现负载倾斜后，将负载迁移到空闲节点
2. 主动预防，防止倾斜的发生
    2.1 系统任务分配方式主动预防
        2.1.1 静态负载均衡
        2.1.2 动态负载均衡
    2.2 用户先验知识的介入
</code></pre>

<p>负载均衡问题的解决在大多数情况下是存在一个极限的，这取决于具体问题的可划分性(<a href="/blog/load-balancing-on-hadoop-mapreduce/#upbound">2.2中对此有讨论</a>)。
分布式系统的负载均衡问题已经研究了多年了，有些问题早有了较成熟的解决方案，像分布式缓存系统中常见的一致性哈希算法等。在这里主要讨论的是分布式计算平台Hadoop里的负载均衡问题。</p>

<p><a href="/blog/analyzing-load-balancing-on-hadoop/">后文</a>会对这几种方式一一讨论，下面先对本文的主要讨论对象Hadoop的背景做个简要介绍。</p>

<!--more-->


<h2>二、Hadoop &ndash; 时代的宠儿</h2>

<p>Hadoop自推出以后在互联网快速发展的背景下得到了许多公司的认可，已然成为大数据的基础处理平台甚至是行业标准。Facebook，Amazon，Yahoo等等公司都在自己的系统中构建了基于Hadoop的处理平台。除了最基本的数据处理功能，在Hadoop之上现在已经发展出来一套生态系统，应用最广泛的莫过于Hive和HBase了。在Hadoop之上构建的系统都直接或间接地使用了了Hadoop的分布式存储模块HDFS和计算模块MapReduce。本文的问题关注计算模块MapReduce的均衡问题。</p>

<h3>2.1 HDFS</h3>

<p>HDFS借鉴的是Google的GFS系统，是一个基于Key/Value的分布式存储系统。HDFS是为了大文件、一次写多次读的应用场景而设计的。所有要存储在HDFS中的文件需要按块（默认64M）切分，每个数据块有在不同的机器上（默认是本机，本机架，不同机架）有多个备份（默认为3份）。系统通过对失败机器数据文件的再分配、复制来自动保证文件的数据安全。HDFS并不适合大量小文件或者对写要求高的场景。这样，我们可以有个概念，Hadoop中处理的数据会分块备份三分存在不同的机器上。</p>

<p>HDFS本身也存在负载均衡问题，这个负载的均衡主要只每台机器的磁盘使用率。假如有一台机器存储了大量的数据，而其他机器存储了很少，这就是一个倾斜的情形。HDFS的存储倾斜不仅仅只影响到磁盘使用情况，同时由于Hadoop的Map的执行依赖于输入数据在磁盘上的分布情况（Hadoop期望达到最好的数据本地化处理）它也影响到Map计算过程中的均衡。HDFS存储的不均很有可能导致Map计算的分布不均（注意是有可能，因为HDFS上的输入数据有多个备份，Map的输入只需要一份备份，因此不一定会导致Map计算不均）。</p>

<p>对于HDFS的倾斜问题，Hadoop本身提供了一套机制来限制不均衡的程度。Hadoop自带的工具<code>bin/start-balancer.sh</code>可以通过参数指定系统中均衡的标准如10%，这就保证了系统中磁盘的使用率的偏差在10%之内。如果超过了这个值，系统将自动执行数据块的重分布工作使之达到偏差限额。</p>

<h3>2.2 MapReduce</h3>

<p>MapReduce是Hadoop的数据处理模块，算是函数式编程的巅峰之作了吧。Hadoop对数据的处理都被抽象成Map和Reduce这两个函数的操作。</p>

<h4>Map</h4>

<p>通常地，Map函数的工作是从HDFS中读取上输入文件，读入的数据是一个个（MapInputKey/MapInputValue）对，根据作业需求处理后输出一个个（MapOutputKey/MapOutputValue)对，后台的输出线程会把输出的文件按照MapOutputKey把对应的MapOutputValue合并起来（MapOutputKey&ndash;>MapOutputValue0,MapOutputValue1,&hellip;)，同时还会将输出按照MapOutputKey排序（注意，每一个Map都会有同样的样的输出，不同的Map会有同样的Key值输出）。逻辑上，我们可以将不同Map输出的同一个Key的数据合起来看做一个小Partition（Finer Partition，FP）。</p>

<h4>Shuffle</h4>

<p>Shuffle被称为Hadoop的核心，但是对应一般用户并不会涉及到这部分的细节。系统提供一个Partition接口使得用户可以决定Map的输出Key该如何聚合（比如想把Key为奇数的FP放在一起，把偶数放在一起）。Shffle的主要工作是将各个Map的FP的各部分按照用户指定的方式将数据从Map的输出端拷贝到对应的Reduce执行端。</p>

<p>在大多数研究中Shuffle的倾斜问题都没有具体考虑，事实上Shuffle这个过程本身也是存在倾斜问题的。主要原因是各个机器上运行的reduce任务的处理数据量的不均。各个机器上运行的Reduce都要从Map端拷贝相应数据，如果这些要拷贝的数据在本地的话那么必然会拷贝得快些（虽然对于本地数据Hadoop的Shuffle还是使用的Http协议向本地Servlet请求下载）；另一方面，一台机器上有多个Reduce在同时下载数据，这台机器的网卡速度及磁盘读写能力都成为这台机器上的Shuffle过程的瓶颈因素。</p>

<p>然而Shuffle这部分倾斜被忽略并不是没有道理的。从企业生产的角度，我们并不关心单个机器的处理时间、通信量。事实上我们最关注的是，对于提交任务的用户而言有最快的响应速度；对整个系统集群而言网络通信量最小，单位时间内处理的任务最多。Shuffle的倾斜问题是中间的一个过渡状态，它是由Map数据的输出不均、任务分配不均导致的；同时它也有可能导致最终Reduce的任务处理时间差异。Shuffle过程中的倾斜并不一定导致最终的倾斜，相反在有些推测执行任务出现的时候，Shuffle的不均有可能还会提升最终性能表现。</p>

<p>总之，Shuffle的均衡既不是目标，也不是高性能的必要条件，因此对于这部分的研究意义不大。</p>

<h4>Reduce</h4>

<p>Reduce将Map输出的各个FP拷贝到本地(拷贝过程中还是保证键值对的有序性)，然后对于每个键值对序列(ReduceInputKey&ndash;> ReduceInputValue0，ReduceInputValue1&hellip;)做处理。对于MapReduce模型本身，如果要保证计算的正确性，我认为至少要保证的是：</p>

<blockquote><p>单独一个键的FP必须要保证完整的拷贝到同一台机器上。而不是看起来的，同一个Hash值对应到的Partition的多个键的FP数据要保证到同一台机器上。</p></blockquote>

<p>多数计算中即使多个键的FP被Shuffle到同一台机器上，处理时我们还是每次以一个键的FP作为独立的计算输入单元。因此，我们在写MapReduce程序时，应当需要注意的是：</p>

<p><span id="upbound"></p>

<blockquote><p>不应当将程序计算的正确性依赖于Partition函数的实现，而只应将Partition函数作为一个提升系统数据均衡性的用户接口。
</span></p></blockquote>

<p>上面这一点也是我们下面提出的各种负载均衡算法的基本依据。如果用户程序不满足上的条件，那么对于这种应用只能做Reduce任务分配级别的均衡，再低层的均衡会影响程序的正确性。而这种问题，Hadoop本身的推测执行机制基本能够满足需求，因此下文不做讨论。这也是均衡算法的能达到的上界(单个Key的FP是Reduce输入数据的最小不可分单元)。在此，下面都假设用户作业满足上面的条件。</p>

<h2>结束语</h2>

<p>本来准备继续写的，但是一篇博文过长感觉不太好，本文先到这里结束。文章简要介绍了负载均衡问题，并针对Hadoop平台上各个部分的运行机制分析了各个部分出现倾斜的情况。指出Hadoop平台上的负载均衡问题重点在Reduce部分。<a href="/blog/analyzing-load-balancing-on-hadoop/">下一篇将详细分析Hadoop倾斜发生的原因及解决方案</a>。</p>
]]></content>
  </entry>
  
</feed>
